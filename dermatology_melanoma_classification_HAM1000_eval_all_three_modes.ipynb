{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGrWVT3k_Zdf",
        "outputId": "bb6d7e81-49f4-4bf8-b704-8a874e55e90c"
      },
      "id": "HGrWVT3k_Zdf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.12.0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5e1edfb7-96fd-465f-a106-1a01e62f903b",
      "metadata": {
        "id": "5e1edfb7-96fd-465f-a106-1a01e62f903b"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# torchvision imports\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.datasets import VisionDataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# PIL imports\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PILSi232K7UA",
        "outputId": "77ce3545-a5a9-4d27-dbad-1b0816b2abe5"
      },
      "id": "PILSi232K7UA",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download and Preprocessing"
      ],
      "metadata": {
        "id": "s4vvac9ZnGFJ"
      },
      "id": "s4vvac9ZnGFJ"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2086d74c-a691-4e32-bf51-1b864f355bd3",
      "metadata": {
        "id": "2086d74c-a691-4e32-bf51-1b864f355bd3"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/dermoscopic_artifacts')\n",
        "sys.path.append('/content/drive/MyDrive/dermoscopic_artifacts')\n",
        "import importlib\n",
        "import datasets\n",
        "importlib.reload(datasets)\n",
        "from datasets import ISICDataset, HAM10000Dataset, PH2Dataset, BCN20000Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d0643793-451a-4846-94cf-dd8fc4018ed2",
      "metadata": {
        "id": "d0643793-451a-4846-94cf-dd8fc4018ed2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c1429c19-f877-4db3-95bf-02d85739ca62",
      "metadata": {
        "id": "c1429c19-f877-4db3-95bf-02d85739ca62"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "src1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\"\n",
        "src2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
        "dst  = \"/kaggle/working/HAM10000_images\"\n",
        "\n",
        "os.makedirs(dst, exist_ok=True)\n",
        "\n",
        "def copy_all(src, dst):\n",
        "    for filename in os.listdir(src):\n",
        "        shutil.copy2(os.path.join(src, filename),\n",
        "                     os.path.join(dst, filename))\n",
        "\n",
        "copy_all(src1, dst)\n",
        "copy_all(src2, dst)\n",
        "\n",
        "print(\"Merged successfully into\", dst)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbYiAl-QHjxA",
        "outputId": "fd568939-4c90-4957-96ea-1298b8e1e3f8"
      },
      "id": "bbYiAl-QHjxA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged successfully into /kaggle/working/HAM10000_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqcogviQEbtp",
        "outputId": "be52d283-8fd5-4f16-eee6-f30f7b0d501e"
      },
      "id": "wqcogviQEbtp",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'skin-cancer-mnist-ham10000' dataset.\n",
            "Path to dataset files: /kaggle/input/skin-cancer-mnist-ham10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebeda74-d1a3-4cb7-a690-1ec6be997927",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6ebeda74-d1a3-4cb7-a690-1ec6be997927"
      },
      "source": [
        "# Eval on HAM10000 - Mode \"whole\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "aea0ac67-6ce5-4f4d-9d81-854566a41354",
      "metadata": {
        "id": "aea0ac67-6ce5-4f4d-9d81-854566a41354"
      },
      "outputs": [],
      "source": [
        "class HAM10000Dataset(Dataset):\n",
        "    def __init__(self, df, image_dir, mask_dir, transform=None, mode=\"whole\", return_pil=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing image names and labels.\n",
        "            image_dir (str): Directory containing original images.\n",
        "            mask_dir (str): Directory containing ground truth segmentations.\n",
        "            transform (callable, optional): Optional transform to apply to images.\n",
        "            mode (str): One of \"whole\", \"lesion\", \"background\", \"bbox\", \"bbox70\",\n",
        "                        \"bbox90\", \"high_whole\", \"low_whole\", \"high_lesion\",\n",
        "                        \"low_lesion\", \"high_background\", \"low_background\".\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.return_pil = return_pil\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_name = self.df.iloc[idx]['image_id']\n",
        "\n",
        "        # le = LabelEncoder()\n",
        "        # df['label'] = le.fit_transform(df['dx'])\n",
        "        # # print(le.classes_)  # saves mapping for inference\n",
        "        # label = self.df.iloc[idx]['label']\n",
        "        # print(label)\n",
        "\n",
        "        # Binary label: 1 = melanoma, 0 = all other classes\n",
        "        df['label'] = (df['dx'] == 'mel').astype(int)\n",
        "\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{img_name}_segmentation.png\")\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load segmentation mask\n",
        "\n",
        "        # Ensure images and masks are the same size\n",
        "        if image.shape[:2] != mask.shape:\n",
        "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Binarize mask\n",
        "        mask = (mask > 0).astype(np.uint8)\n",
        "\n",
        "        if self.mode == \"whole\":\n",
        "            processed_image = image\n",
        "\n",
        "        elif self.mode == \"lesion\":\n",
        "            processed_image = image * mask[:, :, np.newaxis]\n",
        "\n",
        "        elif self.mode == \"background\":\n",
        "            processed_image = image * (1 - mask[:, :, np.newaxis])\n",
        "\n",
        "        elif self.mode in [\"bbox\", \"bbox70\", \"bbox90\"]:\n",
        "            # Compute bounding box around lesion\n",
        "            y_idxs, x_idxs = np.where(mask > 0)\n",
        "            if len(y_idxs) == 0 or len(x_idxs) == 0:  # If no lesion\n",
        "                processed_image = image * 0  # Blackout image\n",
        "            else:\n",
        "                y_min, y_max = y_idxs.min(), y_idxs.max()\n",
        "                x_min, x_max = x_idxs.min(), x_idxs.max()\n",
        "\n",
        "                # Compute the original bbox (for `bbox`)\n",
        "                if self.mode == \"bbox\":\n",
        "                    processed_image = image.copy()\n",
        "                    cv2.rectangle(processed_image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n",
        "\n",
        "                # Expand bbox for bbox70 and bbox90\n",
        "                else:\n",
        "                    expand_ratio = 0.7 if self.mode == \"bbox70\" else 0.9\n",
        "\n",
        "                    img_h, img_w = image.shape[:2]\n",
        "                    bbox_h = y_max - y_min\n",
        "                    bbox_w = x_max - x_min\n",
        "\n",
        "                    # Calculate expansion to reach desired percentage of total image\n",
        "                    target_area = expand_ratio * img_h * img_w\n",
        "                    bbox_center_y, bbox_center_x = (y_min + y_max) // 2, (x_min + x_max) // 2\n",
        "\n",
        "                    # Compute new bbox size\n",
        "                    new_bbox_h = int(np.sqrt(target_area * (bbox_h / bbox_w)))  # Keep aspect ratio\n",
        "                    new_bbox_w = int(np.sqrt(target_area * (bbox_w / bbox_h)))\n",
        "\n",
        "                    # Ensure it fits within image boundaries\n",
        "                    y_min = max(0, bbox_center_y - new_bbox_h // 2)\n",
        "                    y_max = min(img_h, bbox_center_y + new_bbox_h // 2)\n",
        "                    x_min = max(0, bbox_center_x - new_bbox_w // 2)\n",
        "                    x_max = min(img_w, bbox_center_x + new_bbox_w // 2)\n",
        "\n",
        "                    processed_image = image.copy()\n",
        "                    cv2.rectangle(processed_image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n",
        "\n",
        "        elif self.mode.startswith(\"high_\") or self.mode.startswith(\"low_\"):\n",
        "            base_image = None\n",
        "\n",
        "            if \"whole\" in self.mode:\n",
        "                base_image = image\n",
        "            elif \"lesion\" in self.mode:\n",
        "                base_image = image * mask[:, :, np.newaxis]\n",
        "            elif \"background\" in self.mode:\n",
        "                base_image = image * (1 - mask[:, :, np.newaxis])\n",
        "\n",
        "            if base_image is not None:\n",
        "                if \"high_\" in self.mode:\n",
        "                    # processed_image = high_pass_filter(base_image)\n",
        "                    processed_image = high_pass_filter(base_image, sigma=3, grayscale=True)\n",
        "                else:\n",
        "                    processed_image = low_pass_filter(base_image, sigma=3)\n",
        "\n",
        "        if self.return_pil:\n",
        "            processed_image = Image.fromarray(processed_image.astype(np.uint8))\n",
        "        else:\n",
        "            if self.transform:\n",
        "                processed_image = Image.fromarray(processed_image)\n",
        "                processed_image = self.transform(processed_image)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return processed_image, label\n",
        "\n",
        "# Define paths\n",
        "image_dir = \"/kaggle/working/HAM10000_images\"\n",
        "mask_dir = \"/content/drive/MyDrive/dermoscopic_artifacts/HAM10000_segmentations_lesion_tschandl\"\n",
        "\n",
        "# Create dataset instances for each mode\n",
        "dataset_modes = [\"whole\"]\n",
        "#  [\"whole\", \"lesion\", \"background\", \"bbox\", \"bbox70\", \"bbox90\",\n",
        "                #  \"high_whole\", \"low_whole\", \"high_lesion\", \"low_lesion\", \"high_background\", \"low_background\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "60aba34e-03f1-4e64-8390-59310cee2377",
      "metadata": {
        "id": "60aba34e-03f1-4e64-8390-59310cee2377"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "212a28fe-dbce-4691-8446-b590a5f38089",
      "metadata": {
        "id": "212a28fe-dbce-4691-8446-b590a5f38089",
        "outputId": "d58201b3-daa7-4fe1-a733-ed648852b878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10015\n"
          ]
        }
      ],
      "source": [
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "358fd275-30da-404f-bc7f-7b08ff4a19d0",
      "metadata": {
        "id": "358fd275-30da-404f-bc7f-7b08ff4a19d0"
      },
      "outputs": [],
      "source": [
        "dataset_mode = \"whole\"\n",
        "full_dataset = HAM10000Dataset(df, image_dir, mask_dir, transform=transform, mode=dataset_mode, return_pil=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0453b3b4-ba4d-49a0-b499-316e991fd6b6",
      "metadata": {
        "scrolled": true,
        "id": "0453b3b4-ba4d-49a0-b499-316e991fd6b6",
        "outputId": "b0f95749-1e0f-48a2-e78c-e4e6f227cc07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating HAM dataset whole with ISIC Split 1 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Evaluating Split 1: 100%|██████████| 313/313 [1:02:22<00:00, 11.96s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1 - AUROC: 0.7272, Accuracy: 0.7896, Recall: 0.4924, Precision: 0.2622\n",
            "\n",
            "Evaluating HAM dataset whole with ISIC Split 2 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 2: 100%|██████████| 313/313 [05:24<00:00,  1.04s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 2 - AUROC: 0.7662, Accuracy: 0.8247, Recall: 0.4780, Precision: 0.3117\n",
            "\n",
            "Evaluating HAM dataset whole with ISIC Split 3 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 3: 100%|██████████| 313/313 [05:31<00:00,  1.06s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 3 - AUROC: 0.6934, Accuracy: 0.8356, Recall: 0.3684, Precision: 0.3030\n",
            "\n",
            "Evaluating HAM dataset whole with ISIC Split 4 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 4: 100%|██████████| 313/313 [05:27<00:00,  1.05s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 4 - AUROC: 0.7053, Accuracy: 0.8807, Recall: 0.1896, Precision: 0.4187\n",
            "\n",
            "Evaluating HAM dataset whole with ISIC Split 5 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 5: 100%|██████████| 313/313 [05:10<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 5 - AUROC: 0.6737, Accuracy: 0.8531, Recall: 0.2767, Precision: 0.3162\n",
            "\n",
            "===== Final Evaluation Results =====\n",
            "Mean AUROC: 0.7132\n",
            "Mean Accuracy: 0.8367\n",
            "Mean Recall: 0.3610\n",
            "Mean Precision: 0.3224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_mode = \"whole\"  # Change this as needed\n",
        "\n",
        "all_metrics = {\n",
        "    \"AUROC\": [],\n",
        "    \"Accuracy\": [],\n",
        "    \"Recall\": [],\n",
        "    \"Precision\": []\n",
        "}\n",
        "\n",
        "# Directory containing saved models (ISIC models)\n",
        "save_dir = f\"/content/drive/MyDrive/dermoscopic_artifacts/classifiers/{dataset_mode}\"\n",
        "\n",
        "# Load dataset\n",
        "full_ham_dataset = HAM10000Dataset(df, image_dir, mask_dir, transform=transform, mode=dataset_mode, return_pil=False)\n",
        "\n",
        "ham_loader = DataLoader(full_ham_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "store_preds = {}\n",
        "store_labels = {}\n",
        "\n",
        "# Loop through each split\n",
        "for split in range(1, 6):\n",
        "    print(f\"\\nEvaluating HAM dataset {dataset_mode} with ISIC Split {split} model\")\n",
        "    # print(df)\n",
        "\n",
        "    # Get test indices\n",
        "    # print(df)\n",
        "    # test_indices = df[df[f\"split_{split}\"] == \"test\"].index.tolist()\n",
        "\n",
        "    # # Create test dataset and DataLoader\n",
        "    # test_dataset = full_dataset\n",
        "    # # Subset(full_dataset, test_indices)\n",
        "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Load model\n",
        "    model = models.resnet50(pretrained=False)  # Load model architecture\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(num_features, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/resnet50_split_{split}.pth\"))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store predictions and labels\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(ham_loader, desc=f\"Evaluating Split {split}\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.cpu().numpy()  # Convert labels to NumPy array\n",
        "            # print(labels)\n",
        "\n",
        "            outputs = model(images).cpu().numpy()  # Get model predictions\n",
        "            preds = outputs.flatten()  # Flatten predictions\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    auroc = roc_auc_score(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds >= 0.5)\n",
        "    recall = recall_score(all_labels, all_preds >= 0.5)\n",
        "    precision = precision_score(all_labels, all_preds >= 0.5)\n",
        "\n",
        "    # Store metrics\n",
        "    all_metrics[\"AUROC\"].append(auroc)\n",
        "    all_metrics[\"Accuracy\"].append(acc)\n",
        "    all_metrics[\"Recall\"].append(recall)\n",
        "    all_metrics[\"Precision\"].append(precision)\n",
        "\n",
        "    store_preds[split] = all_preds\n",
        "    store_labels[split] = all_labels\n",
        "\n",
        "    print(f\"Split {split} - AUROC: {auroc:.4f}, Accuracy: {acc:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}\")\n",
        "\n",
        "# Compute mean metrics across splits\n",
        "mean_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
        "\n",
        "# Print final results\n",
        "print(\"\\n===== Final Evaluation Results =====\")\n",
        "for metric, mean_value in mean_metrics.items():\n",
        "    print(f\"Mean {metric}: {mean_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwNDiuPfA9wN",
        "outputId": "0bb489c3-101c-40e7-9314-3dd4cbc14bf4"
      },
      "id": "DwNDiuPfA9wN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AUROC': [np.float64(0.7271907864471332),\n",
              "  np.float64(0.7661955186181244),\n",
              "  np.float64(0.6934470443158336),\n",
              "  np.float64(0.7052605156719983),\n",
              "  np.float64(0.673744939152755)],\n",
              " 'Accuracy': [0.7896155766350474,\n",
              "  0.8246630054917623,\n",
              "  0.8356465302046929,\n",
              "  0.8806789815277084,\n",
              "  0.8531203195207189],\n",
              " 'Recall': [0.49236298292902064,\n",
              "  0.4779874213836478,\n",
              "  0.3683737646001797,\n",
              "  0.18957771787960467,\n",
              "  0.27672955974842767],\n",
              " 'Precision': [0.26220095693779905,\n",
              "  0.31165787932044525,\n",
              "  0.30303030303030304,\n",
              "  0.41865079365079366,\n",
              "  0.3162217659137577]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0f21e2-8a52-4b14-8517-c83923cacc68",
      "metadata": {
        "id": "6a0f21e2-8a52-4b14-8517-c83923cacc68"
      },
      "outputs": [],
      "source": [
        "with open (f\"{save_dir}/all_metrics_ham10000.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_metrics, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7de979f-5d9d-4efc-9b57-f1946ba773de",
      "metadata": {
        "id": "b7de979f-5d9d-4efc-9b57-f1946ba773de"
      },
      "outputs": [],
      "source": [
        "with open (f\"{save_dir}/store_preds_ham10000.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store_preds, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea266545-f66b-4c03-9140-016694b0245c",
      "metadata": {
        "id": "ea266545-f66b-4c03-9140-016694b0245c"
      },
      "outputs": [],
      "source": [
        "with open (f\"{save_dir}/store_labels_ham10000.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store_labels, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c384452-1b06-43e4-9720-751095f5e4b4",
      "metadata": {
        "id": "3c384452-1b06-43e4-9720-751095f5e4b4"
      },
      "outputs": [],
      "source": [
        "dataset_modes = [\"whole\"]\n",
        "# , \"lesion\", \"background\", \"bbox\", \"bbox70\", \"bbox90\",\n",
        "#                  \"high_whole\", \"low_whole\", \"high_lesion\", \"low_lesion\", \"high_background\", \"low_background\"]\n",
        "\n",
        "mean_results = {}\n",
        "\n",
        "# Iterate through each artifact and compute mean metrics\n",
        "for mode in dataset_modes:\n",
        "    save_dir = f\"/content/drive/MyDrive/dermoscopic_artifacts/classifiers/{mode}\"\n",
        "    with open(f\"{save_dir}/all_metrics_ham10000.pkl\", \"rb\") as f:\n",
        "        results_dict = pickle.load(f)\n",
        "\n",
        "    # Compute mean for each metric\n",
        "    mean_results[mode] = {metric: np.mean(values) for metric, values in results_dict.items()}\n",
        "\n",
        "# Convert to DataFrame for easy viewing\n",
        "df_mean_results = pd.DataFrame.from_dict(mean_results, orient=\"index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7f0e53-3751-40b5-8426-0902a24a1c6b",
      "metadata": {
        "id": "1e7f0e53-3751-40b5-8426-0902a24a1c6b",
        "outputId": "c12b5a79-aad4-419c-e05d-557ed0a03a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          AUROC  Accuracy    Recall  Precision\n",
              "whole  0.713168  0.836745  0.361006   0.322352"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c548fcf7-93de-4542-9329-aed110791347\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUROC</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>whole</th>\n",
              "      <td>0.713168</td>\n",
              "      <td>0.836745</td>\n",
              "      <td>0.361006</td>\n",
              "      <td>0.322352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c548fcf7-93de-4542-9329-aed110791347')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c548fcf7-93de-4542-9329-aed110791347 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c548fcf7-93de-4542-9329-aed110791347');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_87564c8a-9eb7-4973-89aa-0807c4a28704\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_mean_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_87564c8a-9eb7-4973-89aa-0807c4a28704 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_mean_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_mean_results",
              "summary": "{\n  \"name\": \"df_mean_results\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"AUROC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7131677608411688,\n        \"max\": 0.7131677608411688,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7131677608411688\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.836744882675986,\n        \"max\": 0.836744882675986,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.836744882675986\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.36100628930817613,\n        \"max\": 0.36100628930817613,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.36100628930817613\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3223523397706198,\n        \"max\": 0.3223523397706198,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3223523397706198\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "df_mean_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval on HAM10000 - Mode \"low_background\""
      ],
      "metadata": {
        "id": "IyeIF15MiHA0"
      },
      "id": "IyeIF15MiHA0"
    },
    {
      "cell_type": "code",
      "source": [
        "class HAM10000Dataset(Dataset):\n",
        "    def __init__(self, df, image_dir, mask_dir, transform=None, mode=\"low_background\", return_pil=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing image names and labels.\n",
        "            image_dir (str): Directory containing original images.\n",
        "            mask_dir (str): Directory containing ground truth segmentations.\n",
        "            transform (callable, optional): Optional transform to apply to images.\n",
        "            mode (str): One of \"whole\", \"lesion\", \"background\", \"bbox\", \"bbox70\",\n",
        "                        \"bbox90\", \"high_whole\", \"low_whole\", \"high_lesion\",\n",
        "                        \"low_lesion\", \"high_background\", \"low_background\".\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.return_pil = return_pil\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def low_pass_filter(self, image, sigma=1):\n",
        "      return scipy.ndimage.gaussian_filter(image, sigma=sigma)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_name = self.df.iloc[idx]['image_id']\n",
        "\n",
        "        # le = LabelEncoder()\n",
        "        # df['label'] = le.fit_transform(df['dx'])\n",
        "        # # print(le.classes_)  # saves mapping for inference\n",
        "        # label = self.df.iloc[idx]['label']\n",
        "        # print(label)\n",
        "\n",
        "        # Binary label: 1 = melanoma, 0 = all other classes\n",
        "        df['label'] = (df['dx'] == 'mel').astype(int)\n",
        "\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{img_name}_segmentation.png\")\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load segmentation mask\n",
        "\n",
        "        # Ensure images and masks are the same size\n",
        "        if image.shape[:2] != mask.shape:\n",
        "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Binarize mask\n",
        "        mask = (mask > 0).astype(np.uint8)\n",
        "\n",
        "        if self.mode == \"whole\":\n",
        "            processed_image = image\n",
        "\n",
        "        elif self.mode == \"lesion\":\n",
        "            processed_image = image * mask[:, :, np.newaxis]\n",
        "\n",
        "        elif self.mode == \"background\":\n",
        "            processed_image = image * (1 - mask[:, :, np.newaxis])\n",
        "\n",
        "        elif self.mode in [\"bbox\", \"bbox70\", \"bbox90\"]:\n",
        "            # Compute bounding box around lesion\n",
        "            y_idxs, x_idxs = np.where(mask > 0)\n",
        "            if len(y_idxs) == 0 or len(x_idxs) == 0:  # If no lesion\n",
        "                processed_image = image * 0  # Blackout image\n",
        "            else:\n",
        "                y_min, y_max = y_idxs.min(), y_idxs.max()\n",
        "                x_min, x_max = x_idxs.min(), x_idxs.max()\n",
        "\n",
        "                # Compute the original bbox (for `bbox`)\n",
        "                if self.mode == \"bbox\":\n",
        "                    processed_image = image.copy()\n",
        "                    cv2.rectangle(processed_image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n",
        "\n",
        "                # Expand bbox for bbox70 and bbox90\n",
        "                else:\n",
        "                    expand_ratio = 0.7 if self.mode == \"bbox70\" else 0.9\n",
        "\n",
        "                    img_h, img_w = image.shape[:2]\n",
        "                    bbox_h = y_max - y_min\n",
        "                    bbox_w = x_max - x_min\n",
        "\n",
        "                    # Calculate expansion to reach desired percentage of total image\n",
        "                    target_area = expand_ratio * img_h * img_w\n",
        "                    bbox_center_y, bbox_center_x = (y_min + y_max) // 2, (x_min + x_max) // 2\n",
        "\n",
        "                    # Compute new bbox size\n",
        "                    new_bbox_h = int(np.sqrt(target_area * (bbox_h / bbox_w)))  # Keep aspect ratio\n",
        "                    new_bbox_w = int(np.sqrt(target_area * (bbox_w / bbox_h)))\n",
        "\n",
        "                    # Ensure it fits within image boundaries\n",
        "                    y_min = max(0, bbox_center_y - new_bbox_h // 2)\n",
        "                    y_max = min(img_h, bbox_center_y + new_bbox_h // 2)\n",
        "                    x_min = max(0, bbox_center_x - new_bbox_w // 2)\n",
        "                    x_max = min(img_w, bbox_center_x + new_bbox_w // 2)\n",
        "\n",
        "                    processed_image = image.copy()\n",
        "                    cv2.rectangle(processed_image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n",
        "\n",
        "        elif self.mode.startswith(\"high_\") or self.mode.startswith(\"low_\"):\n",
        "            base_image = None\n",
        "\n",
        "            if \"whole\" in self.mode:\n",
        "                base_image = image\n",
        "            elif \"lesion\" in self.mode:\n",
        "                base_image = image * mask[:, :, np.newaxis]\n",
        "            elif \"background\" in self.mode:\n",
        "                base_image = image * (1 - mask[:, :, np.newaxis])\n",
        "\n",
        "            if base_image is not None:\n",
        "                if \"high_\" in self.mode:\n",
        "                    # processed_image = high_pass_filter(base_image)\n",
        "                    processed_image = high_pass_filter(base_image, sigma=3, grayscale=True)\n",
        "                else:\n",
        "                    processed_image = self.low_pass_filter(base_image, sigma=3)\n",
        "\n",
        "        if self.return_pil:\n",
        "            processed_image = Image.fromarray(processed_image.astype(np.uint8))\n",
        "        else:\n",
        "            if self.transform:\n",
        "                processed_image = Image.fromarray(processed_image)\n",
        "                processed_image = self.transform(processed_image)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return processed_image, label\n",
        "\n",
        "# Define paths\n",
        "image_dir = \"/kaggle/working/HAM10000_images\"\n",
        "mask_dir = \"/content/drive/MyDrive/dermoscopic_artifacts/HAM10000_segmentations_lesion_tschandl\"\n",
        "\n",
        "# Create dataset instances for each mode\n",
        "dataset_modes = [\"low_background\"]\n",
        "#  [\"whole\", \"lesion\", \"background\", \"bbox\", \"bbox70\", \"bbox90\",\n",
        "                #  \"high_whole\", \"low_whole\", \"high_lesion\", \"low_lesion\", \"high_background\", \"low_background\"]"
      ],
      "metadata": {
        "id": "6ufpYcWliIMW"
      },
      "id": "6ufpYcWliIMW",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "eFSQrisCqKsP"
      },
      "id": "eFSQrisCqKsP",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_mode = \"low_background\"\n",
        "full_dataset = HAM10000Dataset(df, image_dir, mask_dir, transform=transform, mode=dataset_mode, return_pil=True)"
      ],
      "metadata": {
        "id": "B4tGbRxWqVf7"
      },
      "id": "B4tGbRxWqVf7",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "all_metrics = {\n",
        "    \"AUROC\": [],\n",
        "    \"Accuracy\": [],\n",
        "    \"Recall\": [],\n",
        "    \"Precision\": []\n",
        "}\n",
        "\n",
        "# Directory containing saved models (ISIC models)\n",
        "save_dir = f\"/content/drive/MyDrive/classifiers/{dataset_mode}\"\n",
        "\n",
        "# Load dataset\n",
        "full_ham_dataset = HAM10000Dataset(df, image_dir, mask_dir, transform=transform, mode=dataset_mode, return_pil=False)\n",
        "\n",
        "ham_loader = DataLoader(full_ham_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "store_preds = {}\n",
        "store_labels = {}\n",
        "\n",
        "# Loop through each split\n",
        "for split in range(1, 6):\n",
        "    print(f\"\\nEvaluating HAM dataset {dataset_mode} with ISIC Split {split} model\")\n",
        "    # print(df)\n",
        "\n",
        "    # Get test indices\n",
        "    # print(df)\n",
        "    # test_indices = df[df[f\"split_{split}\"] == \"test\"].index.tolist()\n",
        "\n",
        "    # # Create test dataset and DataLoader\n",
        "    # test_dataset = full_dataset\n",
        "    # # Subset(full_dataset, test_indices)\n",
        "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Load model\n",
        "    model = models.resnet50(pretrained=False)  # Load model architecture\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(num_features, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/resnet50_split_{split}.pth\"))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store predictions and labels\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(ham_loader, desc=f\"Evaluating Split {split}\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.cpu().numpy()  # Convert labels to NumPy array\n",
        "            # print(labels)\n",
        "\n",
        "            outputs = model(images).cpu().numpy()  # Get model predictions\n",
        "            preds = outputs.flatten()  # Flatten predictions\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    auroc = roc_auc_score(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds >= 0.5)\n",
        "    recall = recall_score(all_labels, all_preds >= 0.5)\n",
        "    precision = precision_score(all_labels, all_preds >= 0.5)\n",
        "\n",
        "    # Store metrics\n",
        "    all_metrics[\"AUROC\"].append(auroc)\n",
        "    all_metrics[\"Accuracy\"].append(acc)\n",
        "    all_metrics[\"Recall\"].append(recall)\n",
        "    all_metrics[\"Precision\"].append(precision)\n",
        "\n",
        "    store_preds[split] = all_preds\n",
        "    store_labels[split] = all_labels\n",
        "\n",
        "    print(f\"Split {split} - AUROC: {auroc:.4f}, Accuracy: {acc:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}\")\n",
        "\n",
        "# Compute mean metrics across splits\n",
        "mean_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
        "\n",
        "# Print final results\n",
        "print(\"\\n===== Final Evaluation Results =====\")\n",
        "for metric, mean_value in mean_metrics.items():\n",
        "    print(f\"Mean {metric}: {mean_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwlEIeAuqYNg",
        "outputId": "fc392757-1ccd-4e05-a5bd-09c9f2beb323"
      },
      "id": "JwlEIeAuqYNg",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating HAM dataset low_background with ISIC Split 1 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Evaluating Split 1: 100%|██████████| 313/313 [10:24<00:00,  1.99s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1 - AUROC: 0.6638, Accuracy: 0.7779, Recall: 0.3801, Precision: 0.2161\n",
            "\n",
            "Evaluating HAM dataset low_background with ISIC Split 2 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 2: 100%|██████████| 313/313 [10:27<00:00,  2.00s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 2 - AUROC: 0.5912, Accuracy: 0.7556, Recall: 0.2740, Precision: 0.1568\n",
            "\n",
            "Evaluating HAM dataset low_background with ISIC Split 3 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 3: 100%|██████████| 313/313 [10:25<00:00,  2.00s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 3 - AUROC: 0.6046, Accuracy: 0.8756, Recall: 0.0665, Precision: 0.2633\n",
            "\n",
            "Evaluating HAM dataset low_background with ISIC Split 4 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 4: 100%|██████████| 313/313 [10:25<00:00,  2.00s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 4 - AUROC: 0.6402, Accuracy: 0.6930, Recall: 0.5031, Precision: 0.1817\n",
            "\n",
            "Evaluating HAM dataset low_background with ISIC Split 5 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 5: 100%|██████████| 313/313 [10:23<00:00,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 5 - AUROC: 0.6710, Accuracy: 0.8037, Recall: 0.3100, Precision: 0.2236\n",
            "\n",
            "===== Final Evaluation Results =====\n",
            "Mean AUROC: 0.6342\n",
            "Mean Accuracy: 0.7811\n",
            "Mean Recall: 0.3067\n",
            "Mean Precision: 0.2083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics"
      ],
      "metadata": {
        "id": "nbS18q4-rwEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12421279-fff3-42a4-e78d-d927845724f4"
      },
      "id": "nbS18q4-rwEW",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AUROC': [np.float64(0.6638349438621161),\n",
              "  np.float64(0.5911587854006983),\n",
              "  np.float64(0.6046240656218063),\n",
              "  np.float64(0.6401880171490987),\n",
              "  np.float64(0.6710381668171522)],\n",
              " 'Accuracy': [0.7779331003494758,\n",
              "  0.7555666500249626,\n",
              "  0.8755866200698952,\n",
              "  0.6929605591612581,\n",
              "  0.8036944583125312],\n",
              " 'Recall': [0.38005390835579517,\n",
              "  0.27403414195867026,\n",
              "  0.0664869721473495,\n",
              "  0.5031446540880503,\n",
              "  0.30997304582210244],\n",
              " 'Precision': [0.21614716402657128,\n",
              "  0.15681233933161953,\n",
              "  0.26334519572953735,\n",
              "  0.18170019467878,\n",
              "  0.2235904082955282]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{save_dir}/all_metrics_ham10000_low_bg.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_metrics, f)"
      ],
      "metadata": {
        "id": "W7EXdSzEsr9d"
      },
      "id": "W7EXdSzEsr9d",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{save_dir}/store_preds_ham10000_low_bg.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store_preds, f)"
      ],
      "metadata": {
        "id": "0ZY2e1Gos2WT"
      },
      "id": "0ZY2e1Gos2WT",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{save_dir}/store_labels_ham10000_low_bg.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store_labels, f)"
      ],
      "metadata": {
        "id": "QPzRmazSs9Ot"
      },
      "id": "QPzRmazSs9Ot",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval on HAM10000 - Mode \"high_lesion\""
      ],
      "metadata": {
        "id": "7D1HqPt_kvde"
      },
      "id": "7D1HqPt_kvde"
    },
    {
      "cell_type": "code",
      "source": [
        "class HAM10000Dataset(Dataset):\n",
        "    def __init__(self, df, image_dir, mask_dir, transform=None, mode=\"high_lesion\", return_pil=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing image names and labels.\n",
        "            image_dir (str): Directory containing original images.\n",
        "            mask_dir (str): Directory containing ground truth segmentations.\n",
        "            transform (callable, optional): Optional transform to apply to images.\n",
        "            mode (str): One of \"whole\", \"lesion\", \"background\", \"bbox\", \"bbox70\",\n",
        "                        \"bbox90\", \"high_whole\", \"low_whole\", \"high_lesion\",\n",
        "                        \"low_lesion\", \"high_background\", \"low_background\".\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.return_pil = return_pil\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def low_pass_filter(self, image, sigma=1):\n",
        "      return scipy.ndimage.gaussian_filter(image, sigma=sigma)\n",
        "\n",
        "    def high_pass_filter(self, image, sigma=1, grayscale=False):\n",
        "      \"\"\"\n",
        "      Apply a high-pass filter to an image.\n",
        "\n",
        "      Args:\n",
        "          image (numpy.ndarray): Input image in RGB format.\n",
        "          sigma (float): Standard deviation for Gaussian blur.\n",
        "          grayscale (bool): If True, converts image to grayscale before filtering.\n",
        "\n",
        "      Returns:\n",
        "          numpy.ndarray: High-pass filtered image.\n",
        "      \"\"\"\n",
        "      if grayscale:\n",
        "          # Convert image to grayscale before filtering (avoids color artifacts)\n",
        "          image_gray = np.dot(image[..., :3], [0.2989, 0.587, 0.114])  # Convert to grayscale\n",
        "          low_frequencies = scipy.ndimage.gaussian_filter(image_gray, sigma=sigma)\n",
        "          high_frequencies = image_gray - low_frequencies\n",
        "          return np.stack([high_frequencies] * 3, axis=-1)  # Expand back to 3 channels for visualization\n",
        "\n",
        "      else:\n",
        "          # Apply filter to each RGB channel separately\n",
        "          high_frequencies = np.zeros_like(image, dtype=np.float32)\n",
        "          for c in range(3):  # Iterate over RGB channels\n",
        "              low_frequencies = scipy.ndimage.gaussian_filter(image[:, :, c], sigma=sigma)\n",
        "              high_frequencies[:, :, c] = image[:, :, c] - low_frequencies\n",
        "\n",
        "          return high_frequencies\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_name = self.df.iloc[idx]['image_id']\n",
        "\n",
        "        # le = LabelEncoder()\n",
        "        # df['label'] = le.fit_transform(df['dx'])\n",
        "        # # print(le.classes_)  # saves mapping for inference\n",
        "        # label = self.df.iloc[idx]['label']\n",
        "        # print(label)\n",
        "\n",
        "        # Binary label: 1 = melanoma, 0 = all other classes\n",
        "        df['label'] = (df['dx'] == 'mel').astype(int)\n",
        "\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{img_name}_segmentation.png\")\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load segmentation mask\n",
        "\n",
        "        # Ensure images and masks are the same size\n",
        "        if image.shape[:2] != mask.shape:\n",
        "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Binarize mask\n",
        "        mask = (mask > 0).astype(np.uint8)\n",
        "\n",
        "        if self.mode == \"whole\":\n",
        "            processed_image = image\n",
        "\n",
        "        elif self.mode == \"lesion\":\n",
        "\n",
        "          #  processed_image = image * mask[:, :, np.newaxis]\n",
        "          mask = mask.astype(np.float32)\n",
        "          processed_image = (image.astype(np.float32) * mask[:, :, None])\n",
        "          processed_image = processed_image.clip(0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "        elif self.mode == \"background\":\n",
        "            processed_image = image * (1 - mask[:, :, np.newaxis])\n",
        "\n",
        "        elif self.mode in [\"bbox\", \"bbox70\", \"bbox90\"]:\n",
        "            # Compute bounding box around lesion\n",
        "            y_idxs, x_idxs = np.where(mask > 0)\n",
        "            if len(y_idxs) == 0 or len(x_idxs) == 0:  # If no lesion\n",
        "                processed_image = image * 0  # Blackout image\n",
        "            else:\n",
        "                y_min, y_max = y_idxs.min(), y_idxs.max()\n",
        "                x_min, x_max = x_idxs.min(), x_idxs.max()\n",
        "\n",
        "                # Compute the original bbox (for `bbox`)\n",
        "                if self.mode == \"bbox\":\n",
        "                    processed_image = image.copy()\n",
        "                    cv2.rectangle(processed_image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n",
        "\n",
        "                # Expand bbox for bbox70 and bbox90\n",
        "                else:\n",
        "                    expand_ratio = 0.7 if self.mode == \"bbox70\" else 0.9\n",
        "\n",
        "                    img_h, img_w = image.shape[:2]\n",
        "                    bbox_h = y_max - y_min\n",
        "                    bbox_w = x_max - x_min\n",
        "\n",
        "                    # Calculate expansion to reach desired percentage of total image\n",
        "                    target_area = expand_ratio * img_h * img_w\n",
        "                    bbox_center_y, bbox_center_x = (y_min + y_max) // 2, (x_min + x_max) // 2\n",
        "\n",
        "                    # Compute new bbox size\n",
        "                    new_bbox_h = int(np.sqrt(target_area * (bbox_h / bbox_w)))  # Keep aspect ratio\n",
        "                    new_bbox_w = int(np.sqrt(target_area * (bbox_w / bbox_h)))\n",
        "\n",
        "                    # Ensure it fits within image boundaries\n",
        "                    y_min = max(0, bbox_center_y - new_bbox_h // 2)\n",
        "                    y_max = min(img_h, bbox_center_y + new_bbox_h // 2)\n",
        "                    x_min = max(0, bbox_center_x - new_bbox_w // 2)\n",
        "                    x_max = min(img_w, bbox_center_x + new_bbox_w // 2)\n",
        "\n",
        "                    processed_image = image.copy()\n",
        "                    cv2.rectangle(processed_image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n",
        "\n",
        "        elif self.mode.startswith(\"high_\") or self.mode.startswith(\"low_\"):\n",
        "            base_image = None\n",
        "\n",
        "            if \"whole\" in self.mode:\n",
        "                base_image = image\n",
        "            elif \"lesion\" in self.mode:\n",
        "                base_image = (image.astype(np.float32) * mask[:, :, None])\n",
        "                # image * mask[:, :, np.newaxis]\n",
        "            elif \"background\" in self.mode:\n",
        "                base_image = image * (1 - mask[:, :, np.newaxis])\n",
        "\n",
        "            if base_image is not None:\n",
        "                if \"high_\" in self.mode:\n",
        "                    # processed_image = self.high_pass_filter(base_image)\n",
        "                    processed_image = self.high_pass_filter(base_image, sigma=3, grayscale=True)\n",
        "                else:\n",
        "                    processed_image = self.low_pass_filter(base_image, sigma=3)\n",
        "\n",
        "        processed_image = np.clip(processed_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "        if self.return_pil:\n",
        "            processed_image = Image.fromarray(processed_image.astype(np.uint8))\n",
        "        else:\n",
        "            if self.transform:\n",
        "                processed_image = Image.fromarray(processed_image)\n",
        "                processed_image = self.transform(processed_image)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return processed_image, label\n",
        "\n",
        "# Define paths\n",
        "image_dir = \"/kaggle/working/HAM10000_images\"\n",
        "mask_dir = \"/content/drive/MyDrive/dermoscopic_artifacts/HAM10000_segmentations_lesion_tschandl\"\n",
        "\n",
        "# Create dataset instances for each mode\n",
        "dataset_modes = [\"high_lesion\"]\n",
        "#  [\"whole\", \"lesion\", \"background\", \"bbox\", \"bbox70\", \"bbox90\",\n",
        "                #  \"high_whole\", \"low_whole\", \"high_lesion\", \"low_lesion\", \"high_background\", \"low_background\"]"
      ],
      "metadata": {
        "id": "AwcPhb-Hkvdf"
      },
      "execution_count": 11,
      "outputs": [],
      "id": "AwcPhb-Hkvdf"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "PV-FxGotkvdf"
      },
      "execution_count": 12,
      "outputs": [],
      "id": "PV-FxGotkvdf"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_mode = \"high_lesion\"\n",
        "full_dataset = HAM10000Dataset(df, image_dir, mask_dir, transform=transform, mode=dataset_mode, return_pil=True)"
      ],
      "metadata": {
        "id": "TiXtQ-Eekvdf"
      },
      "execution_count": 15,
      "outputs": [],
      "id": "TiXtQ-Eekvdf"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "all_metrics = {\n",
        "    \"AUROC\": [],\n",
        "    \"Accuracy\": [],\n",
        "    \"Recall\": [],\n",
        "    \"Precision\": []\n",
        "}\n",
        "\n",
        "# Directory containing saved models (ISIC models)\n",
        "save_dir = f\"/content/drive/MyDrive/dermoscopic_artifacts/classifiers/{dataset_mode}\"\n",
        "\n",
        "# Load dataset\n",
        "full_ham_dataset = HAM10000Dataset(df, image_dir, mask_dir, transform=transform, mode=dataset_mode, return_pil=False)\n",
        "\n",
        "ham_loader = DataLoader(full_ham_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "store_preds = {}\n",
        "store_labels = {}\n",
        "\n",
        "# Loop through each split\n",
        "for split in range(1, 6):\n",
        "    print(f\"\\nEvaluating HAM dataset {dataset_mode} with ISIC Split {split} model\")\n",
        "    # print(df)\n",
        "\n",
        "    # Get test indices\n",
        "    # print(df)\n",
        "    # test_indices = df[df[f\"split_{split}\"] == \"test\"].index.tolist()\n",
        "\n",
        "    # # Create test dataset and DataLoader\n",
        "    # test_dataset = full_dataset\n",
        "    # # Subset(full_dataset, test_indices)\n",
        "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Load model\n",
        "    model = models.resnet50(pretrained=False)  # Load model architecture\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(num_features, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/resnet50_split_{split}.pth\"))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store predictions and labels\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(ham_loader, desc=f\"Evaluating Split {split}\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.cpu().numpy()  # Convert labels to NumPy array\n",
        "            # print(labels)\n",
        "\n",
        "            outputs = model(images).cpu().numpy()  # Get model predictions\n",
        "            preds = outputs.flatten()  # Flatten predictions\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    auroc = roc_auc_score(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds >= 0.5)\n",
        "    recall = recall_score(all_labels, all_preds >= 0.5)\n",
        "    precision = precision_score(all_labels, all_preds >= 0.5)\n",
        "\n",
        "    # Store metrics\n",
        "    all_metrics[\"AUROC\"].append(auroc)\n",
        "    all_metrics[\"Accuracy\"].append(acc)\n",
        "    all_metrics[\"Recall\"].append(recall)\n",
        "    all_metrics[\"Precision\"].append(precision)\n",
        "\n",
        "    store_preds[split] = all_preds\n",
        "    store_labels[split] = all_labels\n",
        "\n",
        "    print(f\"Split {split} - AUROC: {auroc:.4f}, Accuracy: {acc:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}\")\n",
        "\n",
        "# Compute mean metrics across splits\n",
        "mean_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
        "\n",
        "# Print final results\n",
        "print(\"\\n===== Final Evaluation Results =====\")\n",
        "for metric, mean_value in mean_metrics.items():\n",
        "    print(f\"Mean {metric}: {mean_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b98497e-7092-43f1-c6f0-2279d7b2c4d0",
        "id": "B--BP_tQkvdf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating HAM dataset high_lesion with ISIC Split 1 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 1: 100%|██████████| 313/313 [07:39<00:00,  1.47s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1 - AUROC: 0.5837, Accuracy: 0.5977, Recall: 0.5085, Precision: 0.1398\n",
            "\n",
            "Evaluating HAM dataset high_lesion with ISIC Split 2 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 2: 100%|██████████| 313/313 [07:41<00:00,  1.47s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 2 - AUROC: 0.6207, Accuracy: 0.8476, Recall: 0.1456, Precision: 0.2198\n",
            "\n",
            "Evaluating HAM dataset high_lesion with ISIC Split 3 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 3: 100%|██████████| 313/313 [07:45<00:00,  1.49s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 3 - AUROC: 0.6177, Accuracy: 0.5852, Recall: 0.5822, Precision: 0.1494\n",
            "\n",
            "Evaluating HAM dataset high_lesion with ISIC Split 4 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 4: 100%|██████████| 313/313 [07:42<00:00,  1.48s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 4 - AUROC: 0.6253, Accuracy: 0.7663, Recall: 0.3252, Precision: 0.1855\n",
            "\n",
            "Evaluating HAM dataset high_lesion with ISIC Split 5 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Split 5: 100%|██████████| 313/313 [07:47<00:00,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 5 - AUROC: 0.5983, Accuracy: 0.8606, Recall: 0.0889, Precision: 0.2058\n",
            "\n",
            "===== Final Evaluation Results =====\n",
            "Mean AUROC: 0.6091\n",
            "Mean Accuracy: 0.7315\n",
            "Mean Recall: 0.3301\n",
            "Mean Precision: 0.1801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "id": "B--BP_tQkvdf"
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9cf247-d998-4a7d-9c86-933e0f3a901e",
        "id": "QN4uzC-Fkvdg"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AUROC': [np.float64(0.5836651383952606),\n",
              "  np.float64(0.6206949365588722),\n",
              "  np.float64(0.6176939553242525),\n",
              "  np.float64(0.6253030654447762),\n",
              "  np.float64(0.598330871667794)],\n",
              " 'Accuracy': [0.5977034448327508,\n",
              "  0.8476285571642537,\n",
              "  0.5852221667498752,\n",
              "  0.7662506240639041,\n",
              "  0.8606090863704443],\n",
              " 'Recall': [0.5085354896675651,\n",
              "  0.14555256064690028,\n",
              "  0.5822102425876011,\n",
              "  0.3252470799640611,\n",
              "  0.0889487870619946],\n",
              " 'Precision': [0.13982213438735178,\n",
              "  0.2198100407055631,\n",
              "  0.1494120359695642,\n",
              "  0.18545081967213115,\n",
              "  0.20582120582120583]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "id": "QN4uzC-Fkvdg"
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{save_dir}/all_metrics_ham10000_high_lesion.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_metrics, f)"
      ],
      "metadata": {
        "id": "txrYsCfKkvdg"
      },
      "execution_count": 20,
      "outputs": [],
      "id": "txrYsCfKkvdg"
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{save_dir}/store_preds_ham10000_high_lesion.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store_preds, f)"
      ],
      "metadata": {
        "id": "wfnxc5nXkvdg"
      },
      "execution_count": 21,
      "outputs": [],
      "id": "wfnxc5nXkvdg"
    },
    {
      "cell_type": "code",
      "source": [
        "with open (f\"{save_dir}/store_labels_ham10000_high_lesion.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store_labels, f)"
      ],
      "metadata": {
        "id": "t_FnocGMkvdg"
      },
      "execution_count": 22,
      "outputs": [],
      "id": "t_FnocGMkvdg"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_modes = [\"whole\", \"high_lesion\", \"low_background\"]\n",
        "# , \"lesion\", \"background\", \"bbox\", \"bbox70\", \"bbox90\",\n",
        "#                  \"high_whole\", \"low_whole\", \"high_lesion\", \"low_lesion\", \"high_background\", \"low_background\"]\n",
        "\n",
        "mean_results = {}\n",
        "\n",
        "# Iterate through each artifact and compute mean metrics\n",
        "for mode in dataset_modes:\n",
        "    save_dir = f\"/content/drive/MyDrive/dermoscopic_artifacts/classifiers/{mode}\"\n",
        "    with open(f\"{save_dir}/all_metrics_ham10000_{mode}.pkl\", \"rb\") as f:\n",
        "        results_dict = pickle.load(f)\n",
        "\n",
        "    # Compute mean for each metric\n",
        "    mean_results[mode] = {metric: np.mean(values) for metric, values in results_dict.items()}\n",
        "\n",
        "# Convert to DataFrame for easy viewing\n",
        "df_mean_results_ham10000 = pd.DataFrame.from_dict(mean_results, orient=\"index\")"
      ],
      "metadata": {
        "id": "Sd7azkTsOjGk"
      },
      "id": "Sd7azkTsOjGk",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mean_results_ham10000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "bSSi1gJ-O-3w",
        "outputId": "8a269cd1-aa65-4ddd-a907-fc77b94d0bfa"
      },
      "id": "bSSi1gJ-O-3w",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   AUROC  Accuracy    Recall  Precision\n",
              "whole           0.713168  0.836745  0.361006   0.322352\n",
              "high_lesion     0.609138  0.731483  0.330099   0.180063\n",
              "low_background  0.634169  0.781148  0.306739   0.208319"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3cca12e-1a6d-41bb-920d-9342706a5bf0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUROC</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>whole</th>\n",
              "      <td>0.713168</td>\n",
              "      <td>0.836745</td>\n",
              "      <td>0.361006</td>\n",
              "      <td>0.322352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>high_lesion</th>\n",
              "      <td>0.609138</td>\n",
              "      <td>0.731483</td>\n",
              "      <td>0.330099</td>\n",
              "      <td>0.180063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>low_background</th>\n",
              "      <td>0.634169</td>\n",
              "      <td>0.781148</td>\n",
              "      <td>0.306739</td>\n",
              "      <td>0.208319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3cca12e-1a6d-41bb-920d-9342706a5bf0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3cca12e-1a6d-41bb-920d-9342706a5bf0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3cca12e-1a6d-41bb-920d-9342706a5bf0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-32218087-c649-4cce-a077-ace4083cc30d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-32218087-c649-4cce-a077-ace4083cc30d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-32218087-c649-4cce-a077-ace4083cc30d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8bcc8ed2-b241-43b3-aa50-8c63cd099cca\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_mean_results_ham10000')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8bcc8ed2-b241-43b3-aa50-8c63cd099cca button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_mean_results_ham10000');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_mean_results_ham10000",
              "summary": "{\n  \"name\": \"df_mean_results_ham10000\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"AUROC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05429805596894423,\n        \"min\": 0.609137593478191,\n        \"max\": 0.7131677608411688,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7131677608411688,\n          0.609137593478191,\n          0.6341687957701743\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.052658895573232826,\n        \"min\": 0.7314827758362455,\n        \"max\": 0.836744882675986,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.836744882675986,\n          0.7314827758362455,\n          0.7811482775836246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027221199092181982,\n        \"min\": 0.3067385444743936,\n        \"max\": 0.36100628930817613,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.36100628930817613,\n          0.3300988319856244,\n          0.3067385444743936\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07533056662486166,\n        \"min\": 0.18006324731116324,\n        \"max\": 0.3223523397706198,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3223523397706198,\n          0.18006324731116324,\n          0.20831906041240728\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "IyeIF15MiHA0",
        "7D1HqPt_kvde"
      ],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}